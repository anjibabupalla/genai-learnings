{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFNmJpTBhtg-"
      },
      "outputs": [],
      "source": [
        "!pip install openai colorama streamlit beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real World Project: Hacker News ChatBot"
      ],
      "metadata": {
        "id": "pgu2j3Eo5RQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1NxdOPhZ09Td4BwOTz5mKtiZX9xD5cxj5\" alt=\"Alt text\" width=\"500\"/>\n",
        "\n",
        "---\n",
        "\n",
        "In this final lab, we are going to implement the architecture you can see above. It's a ReAct Agent that will choose between three tools (**StoriesTool**, **CommentsTool** and **ContentTool**) to solve the user task.\n",
        "\n",
        "In addition, we'll implement a Streamlit interface, that will enable us to chat with out agent."
      ],
      "metadata": {
        "id": "oF68_nzn5ePX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the `scratch_agent` library"
      ],
      "metadata": {
        "id": "okgdH3DgN5Dg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to define the `scratch_agent` library. This library contains all the code implemented in the previous lessons. This code will be used by the Streamlit application."
      ],
      "metadata": {
        "id": "oOWTYihM8ZLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating parent folder"
      ],
      "metadata": {
        "id": "NHZ5O-sW9c72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p scratch_agent"
      ],
      "metadata": {
        "id": "rDzyIkV887-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating `__init__`"
      ],
      "metadata": {
        "id": "MDcLuCWT9ffR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scratch_agent/__init__.py"
      ],
      "metadata": {
        "id": "8btz6OAi89IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating `utils.py`"
      ],
      "metadata": {
        "id": "tu8vBIhd9ktY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "%%writefile scratch_agent/utils.py\n",
        "\n",
        "\"\"\"\n",
        "This is a collection of helper functions and methods we are going to use in\n",
        "the Agent implementation. You don't need to know the specific implementation\n",
        "of these to follow the Agent code. But, if you are curious, feel free to check\n",
        "them out.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import time\n",
        "\n",
        "from colorama import Fore\n",
        "from colorama import Style\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "def completions_create(client, messages: list, model: str) -> str:\n",
        "    \"\"\"\n",
        "    Sends a request to the client's `completions.create` method to interact with the language model.\n",
        "\n",
        "    Args:\n",
        "        client (OpenAI): The OpenAI client object\n",
        "        messages (list[dict]): A list of message objects containing chat history for the model.\n",
        "        model (str): The model to use for generating tool calls and responses.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the model's response.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(messages=messages, model=model)\n",
        "    return str(response.choices[0].message.content)\n",
        "\n",
        "\n",
        "def build_prompt_structure(prompt: str, role: str, tag: str = \"\") -> dict:\n",
        "    \"\"\"\n",
        "    Builds a structured prompt that includes the role and content.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The actual content of the prompt.\n",
        "        role (str): The role of the speaker (e.g., user, assistant).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary representing the structured prompt.\n",
        "    \"\"\"\n",
        "    if tag:\n",
        "        prompt = f\"<{tag}>{prompt}</{tag}>\"\n",
        "    return {\"role\": role, \"content\": prompt}\n",
        "\n",
        "def update_chat_history(history: list, msg: str, role: str):\n",
        "    \"\"\"\n",
        "    Updates the chat history by appending the latest response.\n",
        "\n",
        "    Args:\n",
        "        history (list): The list representing the current chat history.\n",
        "        msg (str): The message to append.\n",
        "        role (str): The role type (e.g. 'user', 'assistant', 'system')\n",
        "    \"\"\"\n",
        "    history.append(build_prompt_structure(prompt=msg, role=role))\n",
        "\n",
        "\n",
        "class ChatHistory(list):\n",
        "    def __init__(self, messages: list | None = None, total_length: int = -1):\n",
        "        \"\"\"Initialise the queue with a fixed total length.\n",
        "\n",
        "        Args:\n",
        "            messages (list | None): A list of initial messages\n",
        "            total_length (int): The maximum number of messages the chat history can hold.\n",
        "        \"\"\"\n",
        "        if messages is None:\n",
        "            messages = []\n",
        "\n",
        "        super().__init__(messages)\n",
        "        self.total_length = total_length\n",
        "\n",
        "    def append(self, msg: str):\n",
        "        \"\"\"Add a message to the queue.\n",
        "\n",
        "        Args:\n",
        "            msg (str): The message to be added to the queue\n",
        "        \"\"\"\n",
        "        if len(self) == self.total_length:\n",
        "            self.pop(0)\n",
        "        super().append(msg)\n",
        "\n",
        "\n",
        "\n",
        "class FixedFirstChatHistory(ChatHistory):\n",
        "    def __init__(self, messages: list | None = None, total_length: int = -1):\n",
        "        \"\"\"Initialise the queue with a fixed total length.\n",
        "\n",
        "        Args:\n",
        "            messages (list | None): A list of initial messages\n",
        "            total_length (int): The maximum number of messages the chat history can hold.\n",
        "        \"\"\"\n",
        "        super().__init__(messages, total_length)\n",
        "\n",
        "    def append(self, msg: str):\n",
        "        \"\"\"Add a message to the queue. The first messaage will always stay fixed.\n",
        "\n",
        "        Args:\n",
        "            msg (str): The message to be added to the queue\n",
        "        \"\"\"\n",
        "        if len(self) == self.total_length:\n",
        "            self.pop(1)\n",
        "        super().append(msg)\n",
        "\n",
        "def fancy_print(message: str) -> None:\n",
        "    \"\"\"\n",
        "    Displays a fancy print message.\n",
        "\n",
        "    Args:\n",
        "        message (str): The message to display.\n",
        "    \"\"\"\n",
        "    print(Style.BRIGHT + Fore.CYAN + f\"\\n{'=' * 50}\")\n",
        "    print(Fore.MAGENTA + f\"{message}\")\n",
        "    print(Style.BRIGHT + Fore.CYAN + f\"{'=' * 50}\\n\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "\n",
        "def fancy_step_tracker(step: int, total_steps: int) -> None:\n",
        "    \"\"\"\n",
        "    Displays a fancy step tracker for each iteration of the generation-reflection loop.\n",
        "\n",
        "    Args:\n",
        "        step (int): The current step in the loop.\n",
        "        total_steps (int): The total number of steps in the loop.\n",
        "    \"\"\"\n",
        "    fancy_print(f\"STEP {step + 1}/{total_steps}\")\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TagContentResult:\n",
        "    \"\"\"\n",
        "    A data class to represent the result of extracting tag content.\n",
        "\n",
        "    Attributes:\n",
        "        content (List[str]): A list of strings containing the content found between the specified tags.\n",
        "        found (bool): A flag indicating whether any content was found for the given tag.\n",
        "    \"\"\"\n",
        "\n",
        "    content: list[str]\n",
        "    found: bool\n",
        "\n",
        "\n",
        "def extract_tag_content(text: str, tag: str) -> TagContentResult:\n",
        "    \"\"\"\n",
        "    Extracts all content enclosed by specified tags (e.g., <thought>, <response>, etc.).\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input string containing multiple potential tags.\n",
        "        tag (str): The name of the tag to search for (e.g., 'thought', 'response').\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with the following keys:\n",
        "            - 'content' (list): A list of strings containing the content found between the specified tags.\n",
        "            - 'found' (bool): A flag indicating whether any content was found for the given tag.\n",
        "    \"\"\"\n",
        "    # Build the regex pattern dynamically to find multiple occurrences of the tag\n",
        "    tag_pattern = rf\"<{tag}>(.*?)</{tag}>\"\n",
        "\n",
        "    # Use findall to capture all content between the specified tag\n",
        "    matched_contents = re.findall(tag_pattern, text, re.DOTALL)\n",
        "\n",
        "    # Return the dataclass instance with the result\n",
        "    return TagContentResult(\n",
        "        content=[content.strip() for content in matched_contents],\n",
        "        found=bool(matched_contents),\n",
        "    )\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ExZ4pnpN9Aux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating `tools.py`"
      ],
      "metadata": {
        "id": "3DhQyBk09sR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "%%writefile scratch_agent/tools.py\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import Callable\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "def get_fn_signature(fn: Callable) -> dict:\n",
        "    \"\"\"\n",
        "    Generates the signature for a given function.\n",
        "\n",
        "    Args:\n",
        "        fn (Callable): The function whose signature needs to be extracted.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the function's name, description,\n",
        "              and parameter types.\n",
        "    \"\"\"\n",
        "    fn_signature: dict = {\n",
        "        \"name\": fn.__name__,\n",
        "        \"description\": fn.__doc__,\n",
        "        \"parameters\": {\"properties\": {}},\n",
        "    }\n",
        "    schema = {\n",
        "        k: {\"type\": v.__name__} for k, v in fn.__annotations__.items() if k != \"return\"\n",
        "    }\n",
        "    fn_signature[\"parameters\"][\"properties\"] = schema\n",
        "    return fn_signature\n",
        "\n",
        "\n",
        "def validate_arguments(tool_call: dict, tool_signature: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Validates and converts arguments in the input dictionary to match the expected types.\n",
        "\n",
        "    Args:\n",
        "        tool_call (dict): A dictionary containing the arguments passed to the tool.\n",
        "        tool_signature (dict): The expected function signature and parameter types.\n",
        "\n",
        "    Returns:\n",
        "        dict: The tool call dictionary with the arguments converted to the correct types if necessary.\n",
        "    \"\"\"\n",
        "    properties = tool_signature[\"parameters\"][\"properties\"]\n",
        "\n",
        "    # TODO: This is overly simplified but enough for simple Tools.\n",
        "    type_mapping = {\n",
        "        \"int\": int,\n",
        "        \"str\": str,\n",
        "        \"bool\": bool,\n",
        "        \"float\": float,\n",
        "    }\n",
        "\n",
        "    for arg_name, arg_value in tool_call[\"arguments\"].items():\n",
        "        expected_type = properties[arg_name].get(\"type\")\n",
        "\n",
        "        if not isinstance(arg_value, type_mapping[expected_type]):\n",
        "            tool_call[\"arguments\"][arg_name] = type_mapping[expected_type](arg_value)\n",
        "\n",
        "    return tool_call\n",
        "\n",
        "\n",
        "class Tool:\n",
        "    \"\"\"\n",
        "    A class representing a tool that wraps a callable and its signature.\n",
        "\n",
        "    Attributes:\n",
        "        name (str): The name of the tool (function).\n",
        "        fn (Callable): The function that the tool represents.\n",
        "        fn_signature (str): JSON string representation of the function's signature.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name: str, fn: Callable, fn_signature: str):\n",
        "        self.name = name\n",
        "        self.fn = fn\n",
        "        self.fn_signature = fn_signature\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fn_signature\n",
        "\n",
        "    def run(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Executes the tool (function) with provided arguments.\n",
        "\n",
        "        Args:\n",
        "            **kwargs: Keyword arguments passed to the function.\n",
        "\n",
        "        Returns:\n",
        "            The result of the function call.\n",
        "        \"\"\"\n",
        "        return self.fn(**kwargs)\n",
        "\n",
        "\n",
        "def tool(fn: Callable):\n",
        "    \"\"\"\n",
        "    A decorator that wraps a function into a Tool object.\n",
        "\n",
        "    Args:\n",
        "        fn (Callable): The function to be wrapped.\n",
        "\n",
        "    Returns:\n",
        "        Tool: A Tool object containing the function, its name, and its signature.\n",
        "    \"\"\"\n",
        "\n",
        "    def wrapper():\n",
        "        fn_signature = get_fn_signature(fn)\n",
        "        return Tool(\n",
        "            name=fn_signature.get(\"name\"), fn=fn, fn_signature=json.dumps(fn_signature)\n",
        "        )\n",
        "\n",
        "    return wrapper()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VyGH5TJH9LsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating `react.py`"
      ],
      "metadata": {
        "id": "N8NTmbUl91FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "%%writefile scratch_agent/react.py\n",
        "\n",
        "import json\n",
        "import re\n",
        "\n",
        "from colorama import Fore\n",
        "from openai import OpenAI\n",
        "\n",
        "from scratch_agent.tools import tool, Tool, validate_arguments\n",
        "from scratch_agent.utils import ChatHistory, completions_create, extract_tag_content, update_chat_history, build_prompt_structure\n",
        "\n",
        "BASE_SYSTEM_PROMPT = \"\"\n",
        "\n",
        "\n",
        "REACT_SYSTEM_PROMPT = \"\"\"\n",
        "You operate by running a loop with the following steps: Thought, Action, Observation.\n",
        "You are provided with function signatures within <tools></tools> XML tags.\n",
        "You may call one or more functions to assist with the user query. Don' make assumptions about what values to plug\n",
        "into functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\n",
        "\n",
        "For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
        "\n",
        "<tool_call>\n",
        "{\"name\": <function-name>,\"arguments\": <args-dict>, \"id\": <monotonically-increasing-id>}\n",
        "</tool_call>\n",
        "\n",
        "Here are the available tools / actions:\n",
        "\n",
        "<tools>\n",
        "%s\n",
        "</tools>\n",
        "\n",
        "Example session:\n",
        "\n",
        "<question>What's the current temperature in Madrid?</question>\n",
        "<thought>I need to get the current weather in Madrid</thought>\n",
        "<tool_call>{\"name\": \"get_current_weather\",\"arguments\": {\"location\": \"Madrid\", \"unit\": \"celsius\"}, \"id\": 0}</tool_call>\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "<observation>{0: {\"temperature\": 25, \"unit\": \"celsius\"}}</observation>\n",
        "\n",
        "You then output:\n",
        "\n",
        "<response>The current temperature in Madrid is 25 degrees Celsius</response>\n",
        "\n",
        "Additional constraints:\n",
        "\n",
        "- If the user asks you something unrelated to any of the tools above, answer freely enclosing your answer with <response></response> tags.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ReactAgent:\n",
        "    \"\"\"\n",
        "    A class that represents an agent using the ReAct logic that interacts with tools to process\n",
        "    user inputs, make decisions, and execute tool calls. The agent can run interactive sessions,\n",
        "    collect tool signatures, and process multiple tool calls in a given round of interaction.\n",
        "\n",
        "    Attributes:\n",
        "        client (OpenAI): The OpenAI client used to handle model-based completions.\n",
        "        model (str): The name of the model used for generating responses. Default is \"gpt-4o\".\n",
        "        tools (list[Tool]): A list of Tool instances available for execution.\n",
        "        tools_dict (dict): A dictionary mapping tool names to their corresponding Tool instances.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tools: Tool | list[Tool],\n",
        "        model: str = \"gpt-4o\",\n",
        "        system_prompt: str = BASE_SYSTEM_PROMPT,\n",
        "        api_key: str = \"\"\n",
        "    ) -> None:\n",
        "        self.client = OpenAI(\n",
        "            api_key=api_key\n",
        "        )\n",
        "        self.model = model\n",
        "        self.system_prompt = system_prompt\n",
        "        self.tools = tools if isinstance(tools, list) else [tools]\n",
        "        self.tools_dict = {tool.name: tool for tool in self.tools}\n",
        "\n",
        "    def add_tool_signatures(self) -> str:\n",
        "        \"\"\"\n",
        "        Collects the function signatures of all available tools.\n",
        "\n",
        "        Returns:\n",
        "            str: A concatenated string of all tool function signatures in JSON format.\n",
        "        \"\"\"\n",
        "        return \"\".join([tool.fn_signature for tool in self.tools])\n",
        "\n",
        "    def process_tool_calls(self, tool_calls_content: list) -> dict:\n",
        "        \"\"\"\n",
        "        Processes each tool call, validates arguments, executes the tools, and collects results.\n",
        "\n",
        "        Args:\n",
        "            tool_calls_content (list): List of strings, each representing a tool call in JSON format.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary where the keys are tool call IDs and values are the results from the tools.\n",
        "        \"\"\"\n",
        "        observations = {}\n",
        "        for tool_call_str in tool_calls_content:\n",
        "            tool_call = json.loads(tool_call_str)\n",
        "            tool_name = tool_call[\"name\"]\n",
        "            tool = self.tools_dict[tool_name]\n",
        "\n",
        "            print(Fore.GREEN + f\"\\nUsing Tool: {tool_name}\")\n",
        "\n",
        "            # Validate and execute the tool call\n",
        "            validated_tool_call = validate_arguments(\n",
        "                tool_call, json.loads(tool.fn_signature)\n",
        "            )\n",
        "            print(Fore.GREEN + f\"\\nTool call dict: \\n{validated_tool_call}\")\n",
        "\n",
        "            result = tool.run(**validated_tool_call[\"arguments\"])\n",
        "            print(Fore.GREEN + f\"\\nTool result: \\n{result}\")\n",
        "\n",
        "            # Store the result using the tool call ID\n",
        "            observations[validated_tool_call[\"id\"]] = result\n",
        "\n",
        "        return observations\n",
        "\n",
        "    def run(\n",
        "        self,\n",
        "        user_msg: str,\n",
        "        max_rounds: int = 10,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Executes a user interaction session, where the agent processes user input, generates responses,\n",
        "        handles tool calls, and updates chat history until a final response is ready or the maximum\n",
        "        number of rounds is reached.\n",
        "\n",
        "        Args:\n",
        "            user_msg (str): The user's input message to start the interaction.\n",
        "            max_rounds (int, optional): Maximum number of interaction rounds the agent should perform. Default is 10.\n",
        "\n",
        "        Returns:\n",
        "            str: The final response generated by the agent after processing user input and any tool calls.\n",
        "        \"\"\"\n",
        "        user_prompt = build_prompt_structure(\n",
        "            prompt=user_msg, role=\"user\", tag=\"question\"\n",
        "        )\n",
        "        if self.tools:\n",
        "            self.system_prompt += (\n",
        "                \"\\n\" + REACT_SYSTEM_PROMPT % self.add_tool_signatures()\n",
        "            )\n",
        "\n",
        "        chat_history = ChatHistory(\n",
        "            [\n",
        "                build_prompt_structure(\n",
        "                    prompt=self.system_prompt,\n",
        "                    role=\"system\",\n",
        "                ),\n",
        "                user_prompt,\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if self.tools:\n",
        "            # Run the ReAct loop for max_rounds\n",
        "            for _ in range(max_rounds):\n",
        "\n",
        "                completion = completions_create(self.client, chat_history, self.model)\n",
        "\n",
        "                response = extract_tag_content(str(completion), \"response\")\n",
        "                if response.found:\n",
        "                    return response.content[0]\n",
        "\n",
        "                thought = extract_tag_content(str(completion), \"thought\")\n",
        "                tool_calls = extract_tag_content(str(completion), \"tool_call\")\n",
        "\n",
        "                update_chat_history(chat_history, completion, \"assistant\")\n",
        "\n",
        "                print(Fore.MAGENTA + f\"\\nThought: {thought.content[0]}\")\n",
        "\n",
        "                if tool_calls.found:\n",
        "                    observations = self.process_tool_calls(tool_calls.content)\n",
        "                    print(Fore.BLUE + f\"\\nObservations: {observations}\")\n",
        "                    update_chat_history(chat_history, f\"{observations}\", \"user\")\n",
        "\n",
        "        return completions_create(self.client, chat_history, self.model)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nMQE4QGN9QgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the code above, you should see a `scratch_agent` folder in the filesystem, containing all the code implemented in the previous lessons."
      ],
      "metadata": {
        "id": "DIwiYcI68l8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Tools"
      ],
      "metadata": {
        "id": "bC-of0JG61g-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, let's define the three Tools. Before, we need to import the Tool abstractions we implemented in Module 3 (`scratch_agent/tool`).\n",
        "\n",
        "We'll write this file to the parent folder (`./tools.py`) so that we can use it from Streamlit."
      ],
      "metadata": {
        "id": "sJGzqa73ZGAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tools.py\n",
        "\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from scratch_agent.tools import tool\n",
        "\n",
        "BASE_URL = \"https://hacker-news.firebaseio.com/v0\"\n",
        "\n",
        "def fetch_item(item_id: int):\n",
        "    \"\"\"\n",
        "    Fetches details of a story by its ID.\n",
        "\n",
        "    Args:\n",
        "        item_id (int): The ID of the item to fetch.\n",
        "\n",
        "    Returns:\n",
        "        dict: Details of the story.\n",
        "    \"\"\"\n",
        "    url = f\"{BASE_URL}/item/{item_id}.json\"\n",
        "    response = requests.get(url)\n",
        "    return response.json()\n",
        "\n",
        "def fetch_story_ids(story_type: str = \"top\", limit: int = None):\n",
        "    \"\"\"\n",
        "    Fetches the top story IDs.\n",
        "\n",
        "    Args:\n",
        "        story_type: The story type. Defaults to top (`topstories.json`)\n",
        "        limit: The limit of stories to be fetched.\n",
        "\n",
        "    Returns:\n",
        "        List[int]: A list of top story IDs.\n",
        "    \"\"\"\n",
        "    url = f\"{BASE_URL}/{story_type}stories.json\"\n",
        "    response = requests.get(url)\n",
        "    story_ids = response.json()\n",
        "\n",
        "    if limit:\n",
        "        story_ids = story_ids[:limit]\n",
        "\n",
        "    return story_ids\n",
        "\n",
        "def fetch_text(url: str):\n",
        "    \"\"\"\n",
        "    Fetches the text from a URL (if there's text to be fetched). If it fails,\n",
        "    it will return an informative message to the LLM.\n",
        "\n",
        "    Args:\n",
        "        url: The story URL\n",
        "\n",
        "    Returns:\n",
        "        A string representing whether the story text or an informative error (represented as a string)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "\n",
        "            html_content = response.content\n",
        "            soup = BeautifulSoup(html_content, 'html.parser')\n",
        "            text_content = soup.get_text()\n",
        "\n",
        "            return text_content\n",
        "        else:\n",
        "            return f\"Unable to fetch content from {url}. Status code: {response.status}\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "@tool\n",
        "def get_hn_stories(limit: int = 5, story_type: str = \"top\"):\n",
        "    \"\"\"\n",
        "    Fetches the top Hacker News stories based on the provided parameters.\n",
        "\n",
        "    Args:\n",
        "        limit (int): The number of top stories to retrieve. Default is 10.\n",
        "        keywords (List[str]): A list of keywords to filter the top stories.\n",
        "        story_type (str): The story type\n",
        "\n",
        "    Returns:\n",
        "        list[Dict[str, Union[str, int]]]: A list of dictionaries containing\n",
        "        'story_id', 'title', 'url', and 'score' of the stories.\n",
        "    \"\"\"\n",
        "\n",
        "    if limit:\n",
        "        story_ids = fetch_story_ids(story_type, limit)\n",
        "    else:\n",
        "        story_ids = fetch_story_ids(story_type)\n",
        "\n",
        "    def fetch_and_filter_stories(story_id):\n",
        "        return fetch_item(story_id)\n",
        "\n",
        "    stories = [fetch_and_filter_stories(story_id) for story_id in story_ids]\n",
        "    fromatted_stories = []\n",
        "\n",
        "    for story in stories:\n",
        "        story_info = {\n",
        "            \"title\": story.get(\"title\"),\n",
        "            \"url\": story.get(\"url\"),\n",
        "            \"score\": story.get(\"score\"),\n",
        "            \"story_id\": story.get(\"id\"),\n",
        "        }\n",
        "        fromatted_stories.append(story_info)\n",
        "\n",
        "    return fromatted_stories[:limit]\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_relevant_comments(story_id: int, limit: int =10):\n",
        "    \"\"\"\n",
        "    Get the most relevant comments for a Hacker News item.\n",
        "\n",
        "    Args:\n",
        "        story_id: The ID of the Hacker News item.\n",
        "        limit: The number of comments to retrieve (default is 10).\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, each containing comment details.\n",
        "    \"\"\"\n",
        "    story = fetch_item(story_id)\n",
        "\n",
        "    if 'kids' not in story:\n",
        "        return \"This item doesn't have comments.\"\n",
        "\n",
        "    comment_ids = story['kids']\n",
        "\n",
        "    comment_details = [fetch_item(cid) for cid in comment_ids]\n",
        "    comment_details.sort(key=lambda comment: comment.get('score', 0), reverse=True)\n",
        "\n",
        "    relevant_comments = comment_details[:limit]\n",
        "    relevant_comments = [comment[\"text\"] for comment in relevant_comments]\n",
        "\n",
        "    return json.dumps(relevant_comments)\n",
        "\n",
        "@tool\n",
        "def get_story_content(story_url: str):\n",
        "    \"\"\"\n",
        "    Gets the content of the story.\n",
        "\n",
        "    Args:\n",
        "        story_url: A string representing the story URL\n",
        "\n",
        "    Returns:\n",
        "        The content of the story\n",
        "    \"\"\"\n",
        "    return fetch_text(story_url)\n"
      ],
      "metadata": {
        "id": "SwgbUvgDZQ2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the ReAct Agent"
      ],
      "metadata": {
        "id": "AyVby2A6HiTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have all the relevant code from previous modules imported, it's time to define the Agent."
      ],
      "metadata": {
        "id": "WuyjHz8lCA8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hn_bot.py\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "from google.colab import userdata\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from scratch_agent.react import ReactAgent\n",
        "from tools import get_hn_stories, get_relevant_comments, get_story_content\n",
        "\n",
        "def get_hn_bot(api_key: str):\n",
        "  bot_system_prompt = \"\"\"You are the Singularity Incarnation of Hacker News.\n",
        "  The human will ask you for information about Hacker News.\n",
        "  If you can't find any information  about the question asked\n",
        "  or the result is incomplete, apologise to the human and ask him if\n",
        "  you can help him with something else.\n",
        "  If the human asks you to show him stories, do it using a markdown table.\n",
        "  The markdown table has the following format:\n",
        "\n",
        "  story_id | title | url | score\"\"\"\n",
        "\n",
        "  agent = ReactAgent(\n",
        "      system_prompt=bot_system_prompt,\n",
        "      tools=[get_hn_stories, get_relevant_comments, get_story_content],\n",
        "      api_key=api_key\n",
        "  )\n",
        "  return agent"
      ],
      "metadata": {
        "id": "Wp2jPJYQEszb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit Application"
      ],
      "metadata": {
        "id": "sqzQTspsHdja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's create the Streamlit Application. Don't forget to copy your OpenAI API Key, as you'll need it to interact with the app."
      ],
      "metadata": {
        "id": "-UdFLYgI-aJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "\n",
        "from PIL import Image\n",
        "import streamlit as st\n",
        "\n",
        "from hn_bot import get_hn_bot\n",
        "\n",
        "# Set Streamlit page config\n",
        "st.set_page_config(page_title=\"Analytics Vidhya HN Bot 🤖📰\")\n",
        "st.title(\"Analytics Vidhya HN Bot 🤖📰\")\n",
        "\n",
        "\n",
        "# Sidebar - API Key input\n",
        "with st.sidebar:\n",
        "    st.markdown(\"\"\"\n",
        "    # **Greetings, Digital Explorer!**\n",
        "\n",
        "    Are you fatigued from navigating the expansive digital realm in search of your daily tech tales\n",
        "    and hacker happenings? Fear not, for your cyber-savvy companion has descended upon the scene –\n",
        "    behold the extraordinary **Analytics Vidhya HN Bot**!\n",
        "    \"\"\")\n",
        "    api_key = st.text_input(\"Enter your OpenAI API Key:\", type=\"password\")\n",
        "    st.session_state[\"agent\"] = get_hn_bot(api_key=api_key)\n",
        "\n",
        "# Initialize session state\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state[\"messages\"] = []\n",
        "\n",
        "\n",
        "def generate_response(question):\n",
        "    \"\"\"Generate response while passing conversation history as context.\"\"\"\n",
        "    context = \"\\n\".join([msg['bot'] for msg in st.session_state[\"messages\"]])\n",
        "    response = st.session_state[\"agent\"].run(f\"Context: {context} Question: {question}\")\n",
        "    return response\n",
        "\n",
        "# Display chat history\n",
        "for msg in st.session_state[\"messages\"]:\n",
        "    st.chat_message(\"human\").write(msg[\"user\"])\n",
        "    st.chat_message(\"ai\").write(msg[\"bot\"])\n",
        "\n",
        "# Chat input handling\n",
        "if prompt := st.chat_input():\n",
        "    st.chat_message(\"human\").write(prompt)\n",
        "    with st.spinner(\"Thinking ...\"):\n",
        "        response = generate_response(prompt)\n",
        "        st.chat_message(\"ai\").write(response)\n",
        "\n",
        "    # Store conversation in history\n",
        "    st.session_state[\"messages\"].append({\"user\": prompt, \"bot\": response})"
      ],
      "metadata": {
        "id": "hJtdxaLUHF4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "cDCUPj8WJUjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.address=localhost &>/content/logs.txt &\n"
      ],
      "metadata": {
        "id": "BdDSIi3U1L30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "id": "AtNBRwgehqQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "ZqyF4RHPgZT4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}